{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Homework 1\n",
    "<center> Author: Jordy A. Larrea Rodriguez </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the positive proportion of labels. p = # positive labels / # labels\n",
    "def get_binary_p(S: dict):\n",
    "    \n",
    "    labels = np.array(list(S.values()))\n",
    "    return labels.sum()/len(labels)\n",
    "\n",
    "# Returns the dataset Entropy given the proportion of positive labels for a binary classification.\n",
    "def get_entropy(p):\n",
    "    eps = 1e-10\n",
    "    return -p*np.log2(p) - (1-p + eps)*np.log2(1-p + eps)\n",
    "\n",
    "# Returns the information gain for an attribute 'a_i'.\n",
    "def get_info_gain(S, a_i, entropy_S, attribute_counts, plus_counts):\n",
    "    S_length = len(S)\n",
    "    \n",
    "    Sv_lengths = np.array([attribute_counts[v] for v in a_i])\n",
    "    p_counts = np.array([plus_counts[v] for v in a_i])\n",
    "    \n",
    "    length_ratios = Sv_lengths / (np.ones_like(Sv_lengths)*S_length)\n",
    "    Sv_entropies = get_entropy(p_counts/Sv_lengths)\n",
    "    return entropy_S - np.sum(length_ratios*Sv_entropies)\n",
    "\n",
    "# Calculates and returns the information gains (Gain(S,A)) for all attributes a_i in A in a python list.\n",
    "def gain(S, A):\n",
    "    \n",
    "    plus_counts = {}; attribute_counts = {}\n",
    "    for feature, label in S.items():\n",
    "        for v in feature:\n",
    "            plus_counts[v] = plus_counts.get(v, 0) + int(label)\n",
    "            attribute_counts[v] = attribute_counts.get(v, 0) + 1\n",
    "    \n",
    "    H_S = get_entropy(get_binary_p(S))\n",
    " \n",
    "    return [get_info_gain(S, a_i, H_S, attribute_counts, plus_counts) for a_i in A]\n",
    "\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset $S$:\n",
    "$$\n",
    "  \\begin{array}{llll|l}\n",
    "    \\hline\n",
    "    Variety & Color    & Smell  & Time & Ripe?  \\\\ \\hline\n",
    "    Alphonso& Red      & None   & Two  & False  \\\\\n",
    "    Keitt   & Red      & None   & One  & True   \\\\\n",
    "    Alphonso& Yellow   & Sweet  & Two  & True   \\\\\n",
    "    Keitt   & Green    & None   & Two  & False  \\\\\n",
    "    Haden   & Green    & Sweet  & One  & True   \\\\\n",
    "    Alphonso& Yellow   & None   & Two  & False  \\\\\n",
    "    Keitt   & Yellow   & Sweet  & One  & False  \\\\\n",
    "    Alphonso& Red      & Sweet  & Two  & True   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {\n",
    "    ('A', 'R', 'N', 'T') : False,\n",
    "    ('K', 'R', 'N', 'O') : True,\n",
    "    ('A', 'Y', 'S', 'T') : True,\n",
    "    ('K', 'G', 'N', 'T') : False,\n",
    "    ('H', 'G', 'S', 'O') : True,\n",
    "    ('A', 'Y', 'N', 'T') : False,\n",
    "    ('K', 'Y', 'S', 'O') : False,\n",
    "    ('A', 'R', 'S', 'T') : True,\n",
    "}\n",
    "\n",
    "variety = {'A', 'K', 'H'}; color = {'R', 'Y', 'G'}; smell = {'N', 'S'}; time = {'O', 'T'}\n",
    "\n",
    "A = [variety, color, smell, time]; A_names = [\"Variety\", \"Color\", \"Smell\", \"Time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.a.1** - How many possible functions are there to map these four features to\n",
    "a boolean decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: If we consider the feature space to consist of six features, $A = \\{ \\; \\vec{a_1} \\; \\vec{a_2} \\; \\vec{a_3} \\; \\vec{a_4} \\; \\vec{a_5} \\; \\vec{a_6} \\; \\}$ where $\\{ \\; a_1 \\; a_2 \\; \\}$ enumerate the Variety feature and $\\{ \\; a_3 \\; a_4 \\; \\}$ enumerates the Color feature. The entire table for the aforementioned set of features would then result in $2^6 \\; = \\; 64$ rows; thereby, having $2^{64}$ possible solution functions. If we restrict the hypothesis space to only consider possible enumerations, then the resulting table will have $3*3*2*2 \\; = \\; 36$ rows, and $2^{36}$ possible functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.a.2** - How many functions are consistent with the given training\n",
    "dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The dataset has 8 rows; thus, only $2^8 \\; = \\; 256$ functions are consistent w/ the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.b** -  How many functions are consistent with the given training\n",
    "dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy:\n",
    "$$\n",
    "H(S) = -p \\: log_2(p) \\: - \\: (1-p) \\: log_2(1-p) \\text{, where p is the probability of a success.}\n",
    "$$\n",
    "\n",
    "$$p \\: = \\: 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of positive labels is 0.5\n",
      "The entropy of dataset S is 0.9999999999557305.\n"
     ]
    }
   ],
   "source": [
    "# Here p is the probability of + in the dataset and H_S is the entropy of dataset S.\n",
    "print(f\"The proportion of positive labels is {get_binary_p(S)}\")\n",
    "print(f\"The entropy of dataset S is {get_entropy(get_binary_p(S))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.c** - Compute the information gain of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain for a dataset S with attribute/feature $\\vec{a_i}$ $\\in A$:\n",
    "$$\n",
    "Gain(S,\\vec{a_i}) \\; = \\; H(S) - \\Sigma_{v \\in \\vec{a_i}} \\frac{\\lVert S_v \\rVert}{\\lVert S \\rVert} \\, H(S_v) \\text{ , where } S_v \\text{ is the subset of S containing attribute value $v$.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset S: \n",
      "\n",
      "Attributes\t\t | Ripe?\n",
      "_____________________________________________\n",
      "('A', 'R', 'N', 'T')\t | False\n",
      "('K', 'R', 'N', 'O')\t | True\n",
      "('A', 'Y', 'S', 'T')\t | True\n",
      "('K', 'G', 'N', 'T')\t | False\n",
      "('H', 'G', 'S', 'O')\t | True\n",
      "('A', 'Y', 'N', 'T')\t | False\n",
      "('K', 'Y', 'S', 'O')\t | False\n",
      "('A', 'R', 'S', 'T')\t | True\n",
      "\n",
      "Information Gain Table:\n",
      "\n",
      "A\t | Information Gain\n",
      "_____________________________________________\n",
      "Variety\t | 0.1556390618243556\n",
      "Color\t | 0.06127812445276071\n",
      "Smell\t | 0.18872187552011532\n",
      "Time\t | 0.0487949406899022\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset S: \\n\\nAttributes\\t\\t | Ripe?\")\n",
    "print(\"_\"*45)\n",
    "for s in S.items(): print(f\"{s[0]}\\t | {s[1]}\")\n",
    "\n",
    "print(\"\\nInformation Gain Table:\\n\")\n",
    "print(f\"A\\t | Information Gain\")\n",
    "print(\"_\"*45)\n",
    "\n",
    "gain_S = gain(S, A)\n",
    "for i, g_i in enumerate(gain_S):\n",
    "    print(f\"{A_names[i]}\\t | {g_i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.d** - Which attribute will you use to construct the root of the tree using the\n",
    "information gain heuristic of the ID3 algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: I would use the \"Smell\" attribute since the attribute maximizes the gain which minimizes the entropy or disorder in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.e** - Using the root that you selected in the previous question, construct a\n",
    "decision tree that represents the data. You do not have to use the ID3 algorithm\n",
    "here, you can show any tree with the chosen root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q1_dt_CS6350.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.f** - Suppose you are given three more examples. Use\n",
    "your decision tree to predict the label for each example. Also report the accuracy\n",
    "of the classifier that you have learned.\n",
    "\n",
    "Three New Examples:\n",
    "$$\n",
    "  \\begin{array}{llll|l}\n",
    "    \\hline\n",
    "    Variety & Color    & Smell  & Time & Ripe?  \\\\ \\hline\n",
    "    Alphonso& Green   & Sweet  & Two  & True   \\\\\n",
    "    Keitt   & Red    & Sweet   & One  & False  \\\\\n",
    "    Haden   & Yellow    & None  & Two  & True   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: My decision is quite simple given the heuristics, and fits well to the new examples. Because the first two examples have a \"sweet\" smell, they are reduced to a binary decision where the Alphonso variety results in a ripe fruit while the Keitt variety does not. My decision tree assumes that a Haden mango that lacks a smell always results in an unripe fruit which aligns with my decision tree. Thus, my tree has a 100% classification accuracy given the three new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - ID3 Algorithm\n",
    "Recall that in the ID3 algorithm, we want to identify the best attribute\n",
    "that splits the examples that are relatively pure in one label. Aside from entropy,\n",
    "which we saw in class and you used in the previous question, there are other methods\n",
    "to measure impurity.\n",
    "We will now develop a variant of the ID3 algorithm that does not use entropy. If,\n",
    "at some node, we stopped growing the tree and assign the most common label of the\n",
    "remaining examples at that node, then the empirical error on the training set $S$ at that\n",
    "node will be\n",
    "$$\n",
    "ME(S) = 1 \\: - \\: \\underset{i}{max} \\, p_i \\; \\text{, where } p_i \\text{ is the fraction of examples that are labeled with the } i^{th} \\text{ label.}\n",
    "$$\n",
    "Furthermore, $ME$ can be thought as the minimum proportion for the binary labels. That is...\n",
    "$$\n",
    "ME(S)= min(p_+, p_-) = min(p, 1-p) \\; \\text{, where } p \\text{ is the fraction of examples that are + labeled.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a** -  Notice that $MajorityError$ can be thought of as a measure of impurity\n",
    "just like entropy. Just like we used entropy to define information gain, we can\n",
    "define a new version of information gain that uses $MajorityError$ ($ME$) in place of\n",
    "entropy. Write down an expression that defines a new version of information gain\n",
    "that uses $MajorityError$ in place of entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain for a dataset S with attribute/feature $\\vec{a_i}$ $\\in A$:\n",
    "$$\n",
    "Gain_{ME}(S,\\vec{a_i}) \\; = \\; ME(S) - \\Sigma_{v \\in \\vec{a_i}} \\frac{\\lVert S_v \\rVert}{\\lVert S \\rVert} \\, ME(S_v) \\text{ , where } S_v \\text{ is the subset of S containing attribute value $v$.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b** - Calculate the value of your newly defined information gain from the\n",
    "previous question for the four features in the mango dataset.\n",
    "\n",
    "$$\n",
    "ME = min(p, 1-p) = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c|c}\n",
    "  \\hline\n",
    "  Feature & \\text{Information Gain (using majority error)} \\\\ \\hline\n",
    "  Variety & G(S, variety) = \\frac{1}{2} - (\\frac{1}{2}(\\frac{1}{2}) + \\frac{3}{8}(\\frac{1}{3}) + \\frac{1}{8}(0))=\\frac{1}{8} \\\\\n",
    "  Color   & G(S, variety) = \\frac{1}{2} - (\\frac{3}{8}(\\frac{1}{3}) + \\frac{3}{8}(\\frac{1}{3}) + \\frac{1}{4}(\\frac{1}{2}))=\\frac{1}{8} \\\\\n",
    "  Smell   & G(S, variety) = \\frac{1}{2} - (\\frac{1}{2}(\\frac{1}{4}) + \\frac{1}{2}(\\frac{1}{4}))=\\frac{1}{4} \\\\\n",
    "  Time    & G(S, variety) = \\frac{1}{2} - (\\frac{3}{8}(\\frac{1}{3}) + \\frac{5}{8}(\\frac{2}{5}))=\\frac{1}{8} \\\\ \\hline\n",
    "\\end{array}\n",
    "$$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c** - According to your results in the last question, which attribute should\n",
    "be the root for the decision tree? Do these two measures (entropy and majority\n",
    "error) lead to the same tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The root node should be \"Smell\" since it maximizes the gain; however, because the other attributes produce the same gain of $\\frac{1}{8}$, the resulting tree would depend on the tie-breaking strategy employed. Therefore, the $ME$ heuristic might lead to the same tree, but it is not guaranteed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cade_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Homework 1\n",
    "<center> Author: Jordy A. Larrea Rodriguez </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the positive proportion of labels. p = # positive labels / # labels\n",
    "def get_binary_p(S: dict):\n",
    "    \n",
    "    labels = np.array(list(S.values()))\n",
    "    return labels.sum()/len(labels)\n",
    "\n",
    "# Returns the dataset Entropy given the proportion of positive labels for a binary classification.\n",
    "def get_entropy(p):\n",
    "    eps = 1e-10\n",
    "    return -p*np.log2(p) - (1-p + eps)*np.log2(1-p + eps)\n",
    "\n",
    "# Returns the information gain for an attribute 'a_i'.\n",
    "def get_info_gain(S, a_i, entropy_S, attribute_counts, plus_counts):\n",
    "    S_length = len(S)\n",
    "    \n",
    "    Sv_lengths = np.array([attribute_counts[v] for v in a_i])\n",
    "    p_counts = np.array([plus_counts[v] for v in a_i])\n",
    "    \n",
    "    length_ratios = Sv_lengths / (np.ones_like(Sv_lengths)*S_length)\n",
    "    Sv_entropies = get_entropy(p_counts/Sv_lengths)\n",
    "    return entropy_S - np.sum(length_ratios*Sv_entropies)\n",
    "\n",
    "# Calculates and returns the information gains (Gain(S,A)) for all attributes a_i in A in a python list.\n",
    "def gain(S, A):\n",
    "    \n",
    "    plus_counts = {}; attribute_counts = {}\n",
    "    for feature, label in S.items():\n",
    "        for v in feature:\n",
    "            plus_counts[v] = plus_counts.get(v, 0) + int(label)\n",
    "            attribute_counts[v] = attribute_counts.get(v, 0) + 1\n",
    "    \n",
    "    H_S = get_entropy(get_binary_p(S))\n",
    " \n",
    "    return [get_info_gain(S, a_i, H_S, attribute_counts, plus_counts) for a_i in A]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset $S$:\n",
    "$$\n",
    "  \\begin{array}{llll|l}\n",
    "    \\hline\n",
    "    Variety & Color    & Smell  & Time & Ripe?  \\\\ \\hline\n",
    "    Alphonso& Red      & None   & Two  & False  \\\\\n",
    "    Keitt   & Red      & None   & One  & True   \\\\\n",
    "    Alphonso& Yellow   & Sweet  & Two  & True   \\\\\n",
    "    Keitt   & Green    & None   & Two  & False  \\\\\n",
    "    Haden   & Green    & Sweet  & One  & True   \\\\\n",
    "    Alphonso& Yellow   & None   & Two  & False  \\\\\n",
    "    Keitt   & Yellow   & Sweet  & One  & False  \\\\\n",
    "    Alphonso& Red      & Sweet  & Two  & True   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {\n",
    "    ('A', 'R', 'N', 'T') : False,\n",
    "    ('K', 'R', 'N', 'O') : True,\n",
    "    ('A', 'Y', 'S', 'T') : True,\n",
    "    ('K', 'G', 'N', 'T') : False,\n",
    "    ('H', 'G', 'S', 'O') : True,\n",
    "    ('A', 'Y', 'N', 'T') : False,\n",
    "    ('K', 'Y', 'S', 'O') : False,\n",
    "    ('A', 'R', 'S', 'T') : True,\n",
    "}\n",
    "\n",
    "variety = {'A', 'K', 'H'}; color = {'R', 'Y', 'G'}; smell = {'N', 'S'}; time = {'O', 'T'}\n",
    "\n",
    "A = [variety, color, smell, time]; A_names = [\"Variety\", \"Color\", \"Smell\", \"Time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a.1** - How many possible functions are there to map these four features to\n",
    "a boolean decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: If we consider the feature space to consist of six features, $A = \\{ \\; \\vec{a_1} \\; \\vec{a_2} \\; \\vec{a_3} \\; \\vec{a_4} \\; \\vec{a_5} \\; \\vec{a_6} \\; \\}$ where $\\{ \\; a_1 \\; a_2 \\; \\}$ enumerate the Variety feature and $\\{ \\; a_3 \\; a_4 \\; \\}$ enumerates the Color feature. The entire table for the aforementioned set of features would then result in $2^6 \\; = \\; 64$ rows; thereby, having $2^{64}$ possible solution functions. If we restrict the hypothesis space to only consider possible enumerations, then the resulting table will have $3*3*2*2 \\; = \\; 36$ rows, and $2^{36}$ possible functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a.2** - How many functions are consistent with the given training\n",
    "dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The dataset has 8 rows; thus, only $2^8 \\; = \\; 256$ functions are consistent w/ the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b** -  How many functions are consistent with the given training\n",
    "dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy:\n",
    "$$\n",
    "H(S) = -p \\: log_2(p) \\: - \\: (1-p) \\: log_2(1-p) \\text{, where p is the probability of a success.}\n",
    "$$\n",
    "\n",
    "$$p \\: = \\: 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of positive labels is 0.5\n",
      "The entropy of dataset S is 0.9999999999557305.\n"
     ]
    }
   ],
   "source": [
    "# Here p is the probability of + in the dataset and H_S is the entropy of dataset S.\n",
    "print(f\"The proportion of positive labels is {get_binary_p(S)}\")\n",
    "print(f\"The entropy of dataset S is {get_entropy(get_binary_p(S))}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.c** - Compute the information gain of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain for a dataset S with attribute/feature $\\vec{a_i}$ $\\in A$:\n",
    "$$\n",
    "Gain(S,\\vec{a_i}) \\; = \\; H(S) - \\Sigma_{v \\in \\vec{a_i}} \\frac{\\lVert S_v \\rVert}{\\lVert S \\rVert} \\, H(S_v) \\text{ , where } S_v \\text{ is the subset of S containing attribute value $v$.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset S: \n",
      "\n",
      "Attributes\t\t | Ripe?\n",
      "_____________________________________________\n",
      "('A', 'R', 'N', 'T')\t | False\n",
      "('K', 'R', 'N', 'O')\t | True\n",
      "('A', 'Y', 'S', 'T')\t | True\n",
      "('K', 'G', 'N', 'T')\t | False\n",
      "('H', 'G', 'S', 'O')\t | True\n",
      "('A', 'Y', 'N', 'T')\t | False\n",
      "('K', 'Y', 'S', 'O')\t | False\n",
      "('A', 'R', 'S', 'T')\t | True\n",
      "\n",
      "Information Gain Table:\n",
      "\n",
      "A\t | Information Gain\n",
      "_____________________________________________\n",
      "Variety\t | 0.1556390618243556\n",
      "Color\t | 0.06127812445276071\n",
      "Smell\t | 0.18872187552011532\n",
      "Time\t | 0.0487949406899022\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset S: \\n\\nAttributes\\t\\t | Ripe?\")\n",
    "print(\"_\"*45)\n",
    "for s in S.items(): print(f\"{s[0]}\\t | {s[1]}\")\n",
    "\n",
    "print(\"\\nInformation Gain Table:\\n\")\n",
    "print(f\"A\\t | Information Gain\")\n",
    "print(\"_\"*45)\n",
    "\n",
    "gain_S = gain(S, A)\n",
    "for i, g_i in enumerate(gain_S):\n",
    "    print(f\"{A_names[i]}\\t | {g_i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.d** - Which attribute will you use to construct the root of the tree using the\n",
    "information gain heuristic of the ID3 algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: I would use the \"Smell\" attribute since the attribute maximizes the gain which minimizes the entropy or disorder in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.e** - Using the root that you selected in the previous question, construct a\n",
    "decision tree that represents the data. You do not have to use the ID3 algorithm\n",
    "here, you can show any tree with the chosen root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/q1_dt_CS6350.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : for some reason my image is not showing up on the printout. Please refer to the hw1/img/ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.f** - Suppose you are given three more examples. Use\n",
    "your decision tree to predict the label for each example. Also report the accuracy\n",
    "of the classifier that you have learned.\n",
    "\n",
    "Three New Examples:\n",
    "$$\n",
    "  \\begin{array}{llll|l}\n",
    "    \\hline\n",
    "    Variety & Color    & Smell  & Time & Ripe?  \\\\ \\hline\n",
    "    Alphonso& Green   & Sweet  & Two  & True   \\\\\n",
    "    Keitt   & Red    & Sweet   & One  & False  \\\\\n",
    "    Haden   & Yellow    & None  & Two  & True   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: My decision is quite simple given the heuristics, and fits well to the new examples. Because the first two examples have a \"sweet\" smell, they are reduced to a binary decision where the Alphonso variety results in a ripe fruit while the Keitt variety does not. My decision tree assumes that a Haden mango that lacks a smell always results in an unripe fruit which aligns with my decision tree. Thus, my tree has a 100% classification accuracy given the three new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - ID3 Algorithm\n",
    "Recall that in the ID3 algorithm, we want to identify the best attribute\n",
    "that splits the examples that are relatively pure in one label. Aside from entropy,\n",
    "which we saw in class and you used in the previous question, there are other methods\n",
    "to measure impurity.\n",
    "We will now develop a variant of the ID3 algorithm that does not use entropy. If,\n",
    "at some node, we stopped growing the tree and assign the most common label of the\n",
    "remaining examples at that node, then the empirical error on the training set $S$ at that\n",
    "node will be\n",
    "$$\n",
    "ME(S) = 1 \\: - \\: \\underset{i}{max} \\, p_i \\; \\text{, where } p_i \\text{ is the fraction of examples that are labeled with the } i^{th} \\text{ label.}\n",
    "$$\n",
    "Furthermore, $ME$ can be thought as the minimum proportion for the binary labels. That is...\n",
    "$$\n",
    "ME(S)= min(p_+, p_-) = min(p, 1-p) \\; \\text{, where } p \\text{ is the fraction of examples that are + labeled.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a** -  Notice that $MajorityError$ can be thought of as a measure of impurity\n",
    "just like entropy. Just like we used entropy to define information gain, we can\n",
    "define a new version of information gain that uses $MajorityError$ ($ME$) in place of\n",
    "entropy. Write down an expression that defines a new version of information gain\n",
    "that uses $MajorityError$ in place of entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain for a dataset S with attribute/feature $\\vec{a_i}$ $\\in A$:\n",
    "$$\n",
    "Gain_{ME}(S,\\vec{a_i}) \\; = \\; ME(S) - \\Sigma_{v \\in \\vec{a_i}} \\frac{\\lVert S_v \\rVert}{\\lVert S \\rVert} \\, ME(S_v) \\text{ , where } S_v \\text{ is the subset of S containing attribute value $v$.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b** - Calculate the value of your newly defined information gain from the\n",
    "previous question for the four features in the mango dataset.\n",
    "\n",
    "$$\n",
    "ME = min(p, 1-p) = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c|c}\n",
    "  \\hline\n",
    "  Feature & \\text{Information Gain (using majority error)} \\\\ \\hline\n",
    "  Variety & G(S, variety) = \\frac{1}{2} - (\\frac{1}{2}(\\frac{1}{2}) + \\frac{3}{8}(\\frac{1}{3}) + \\frac{1}{8}(0))=\\frac{1}{8} \\\\\n",
    "  Color   & G(S, variety) = \\frac{1}{2} - (\\frac{3}{8}(\\frac{1}{3}) + \\frac{3}{8}(\\frac{1}{3}) + \\frac{1}{4}(\\frac{1}{2}))=\\frac{1}{8} \\\\\n",
    "  Smell   & G(S, variety) = \\frac{1}{2} - (\\frac{1}{2}(\\frac{1}{4}) + \\frac{1}{2}(\\frac{1}{4}))=\\frac{1}{4} \\\\\n",
    "  Time    & G(S, variety) = \\frac{1}{2} - (\\frac{3}{8}(\\frac{1}{3}) + \\frac{5}{8}(\\frac{2}{5}))=\\frac{1}{8} \\\\ \\hline\n",
    "\\end{array}\n",
    "$$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c** - According to your results in the last question, which attribute should\n",
    "be the root for the decision tree? Do these two measures (entropy and majority\n",
    "error) lead to the same tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The root node should be \"Smell\" since it maximizes the gain; however, because the other attributes produce the same gain of $\\frac{1}{8}$, the resulting tree would depend on the tie-breaking strategy employed. Therefore, the $ME$ heuristic might lead to the same tree, but it is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Setup\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import load_data, get_attribute_dict, calc_acc\n",
    "from decision_tree import DecisionTree\n",
    "\n",
    "# load data\n",
    "labels = {True:'p', False:'e'}\n",
    "\n",
    "x_train, y_train, data_train = load_data(list(labels.values()), null='?', dir=r'data/train.csv')\n",
    "x_test, y_test, data_test = load_data(list(labels.values()), null='?', dir=r'data/test.csv')\n",
    "\n",
    "attribute_dict = get_attribute_dict(pd.concat([x_train, x_test]), [('veil-type','u')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a** - First, find the most common label in the training data. What is the training and test\n",
    "accuracy of a classifier that always predicts this label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'poisonous' and 'edible' labels occur 3148 and 3382 times respectively\n",
      "The most common label in this training data is 'e'.\n",
      "\n",
      "A classifier that always predicts 'e' gets an accuracy of 51.792% on the train set.\n",
      "A classifier that always predicts 'e' gets an accuracy of 51.819% on the test set.\n"
     ]
    }
   ],
   "source": [
    "p_count_train = len(np.where(y_train)[0]); e_count_train = len(np.where(~y_train)[0])\n",
    "p_count_test = len(np.where(y_test)[0]); e_count_test = len(np.where(~y_test)[0])\n",
    "print(f\"The 'poisonous' and 'edible' labels occur {p_count_train} and {e_count_train} times respectively\")\n",
    "print(f\"The most common label in this training data is '{labels[p_count_train > e_count_train]}'.\")\n",
    "print(f\"\\nA classifier that always predicts 'e' gets an accuracy of {round(e_count_train/(e_count_train+p_count_train)*100,3)}% on the train set.\")\n",
    "print(f\"A classifier that always predicts 'e' gets an accuracy of {round(e_count_test/(e_count_test+p_count_test)*100,3)}% on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - Full Tree Implementation\n",
    "In the first set of decision tree experiments, run the ID3 algorithm we saw in class\n",
    "without any depth restrictions. (That is, there are no hyperparameters for this setting.)\n",
    "[6 points] Implement the decision tree data structure and the ID3 algorithm for your\n",
    "decision tree (Remember that the decision tree need not be a binary tree!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tree for Q2 on Provided Datasets\n",
    "q2_test = DecisionTree(a_dict=attribute_dict)\n",
    "q2_test.train(x_train, y_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a** - The root feature that is selected by your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root attribute selected by my algorithm is: 'spore-print-color'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The root attribute selected by my algorithm is: '{q2_test.get_max_gain()[0]}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b** - Information gain for the root feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information gain of the root feature is: 0.485.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The information gain of the root feature is: {round(q2_test.get_max_gain()[1], 3)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c** - Maximum depth of the tree that your implementation gives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The height of my tree via DFS traversal: 12\n",
      "The height of my tree via BFS traversal: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"The height of my tree via DFS traversal: {q2_test.depth}\")\n",
    "print(f\"The height of my tree via BFS traversal: {q2_test.calculate_tree_depth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.d** - Accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy of my classifier on the training set is 100.0%.\n"
     ]
    }
   ],
   "source": [
    "train_pred = q2_test.predict(x_train)\n",
    "print(f\"The prediction accuracy of my classifier on the training set is {round(calc_acc(train_pred, y_train)*100, 3)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.e** - Accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy of my classifier on the testing set is 100.0%.\n"
     ]
    }
   ],
   "source": [
    "test_pred = q2_test.predict(x_test)\n",
    "print(f\"The prediction accuracy of my classifier on the testing set is {round(calc_acc(test_pred, y_test)*100, 3)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Limiting Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will perform 5-fold cross-validation to limit the depth of your decision tree,\n",
    "effectively pruning the tree to avoid overfitting. We have already randomly split the\n",
    "training data into five splits. You should use the 5 cross-validation files for this section,\n",
    "titled data/CVfolds/foldX.csv where X is a number between 1 and 5 (inclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.a** - Run 5-fold cross-validation using the specified files. Experiment with\n",
    "depths in the set {1, 2, 3, 4, 5, 10, 15}, reporting the average cross-validation accuracy and standard deviation for each depth. Explicitly specify which depth should\n",
    "be chosen as the best, and explain why. If a certain depth is not feasible for any\n",
    "reason, your report should explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, calc_acc\n",
    "from decision_tree import DecisionTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set for Depth Hyperparameter\n",
    "max_depths = {1, 2, 3, 4, 5, 10, 15}\n",
    "labels = {True:'p', False:'e'}\n",
    "\n",
    "# Load K-fold datasets\n",
    "K = 5\n",
    "k_datasets = [load_data(list(labels.values()), null='?', dir=f'./data/CVfolds_new/fold{k}.csv')[2] for k in range(1, K+1)]\n",
    "dataset = pd.concat(k_datasets)\n",
    "attribute_dict = get_attribute_dict(dataset.loc[:, ~dataset.columns.isin(['label'])], [('veil-type','u')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five-Fold Cross-Validation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from five-fold cross-validation:\n",
      "\n",
      "depth 'd'\t| five-fold cross-validation Accuracies\n",
      "d=1\t\t| [0.116 0.112 0.335 0.169 0.312]\n",
      "d=2\t\t| [0.116 0.117 0.335 0.169 0.312]\n",
      "d=3\t\t| [0.116 0.132 0.547 0.685 0.312]\n",
      "d=4\t\t| [0.152 0.304 0.547 0.828 0.765]\n",
      "d=5\t\t| [0.162 0.305 0.771 0.828 0.75 ]\n",
      "d=10\t\t| [0.946 1.    0.985 0.974 0.975]\n",
      "d=15\t\t| [0.949 1.    0.985 0.974 0.975]\n"
     ]
    }
   ],
   "source": [
    "cv_acc = {md : [] for md in max_depths}\n",
    "\n",
    "# 5-fold cross-validation\n",
    "for md in max_depths:\n",
    "    for k in range(K):\n",
    "        k_ds = list(k_datasets)\n",
    "\n",
    "        # Derive validation set\n",
    "        data_val = k_ds[k]\n",
    "        x_val = data_val.loc[:, ~data_val.columns.isin(['label'])]\n",
    "        y_val = (data_val.loc[:, data_val.columns.isin(['label'])].to_numpy() == list(labels.values())[0]).flatten()\n",
    "\n",
    "        k_ds.pop(k)\n",
    "\n",
    "        # Derive training set\n",
    "        data_train = pd.concat(k_ds)\n",
    "        x_train = data_train.loc[:, ~data_train.columns.isin(['label'])]\n",
    "        y_train = (data_train.loc[:, data_train.columns.isin(['label'])].to_numpy() == list(labels.values())[0]).flatten()\n",
    "\n",
    "\n",
    "        dt = DecisionTree(a_dict=attribute_dict)\n",
    "        dt.train(x_train, y_train, labels, max_height=md)\n",
    "\n",
    "        cv_acc[md].append(calc_acc(dt.predict(x_val), y_val))\n",
    "        \n",
    "print(\"Results from five-fold cross-validation:\\n\")\n",
    "print(\"depth 'd'\\t| five-fold cross-validation Accuracies\")\n",
    "for md, trials in cv_acc.items():\n",
    "    print(f\"d={md}\\t\\t| {np.round(trials,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy statistics from five-fold cross-validation:\n",
      "\n",
      "depth 'd' \t| five-fold mean | five-fold std\n",
      "d=1\t\t| 0.209\t\t | 0.096\n",
      "d=2\t\t| 0.21\t\t | 0.095\n",
      "d=3\t\t| 0.358\t\t | 0.226\n",
      "d=4\t\t| 0.519\t\t | 0.26\n",
      "d=5\t\t| 0.563\t\t | 0.274\n",
      "d=10\t\t| 0.976\t\t | 0.018\n",
      "d=15\t\t| 0.977\t\t | 0.016\n"
     ]
    }
   ],
   "source": [
    "# Take Average across Five-Fold Split Trials\n",
    "\n",
    "cv_acc_stats = {md : (np.mean(trials), np.std(trials)) for md, trials in cv_acc.items()}\n",
    "\n",
    "print(\"Accuracy statistics from five-fold cross-validation:\\n\")\n",
    "print(\"depth 'd' \\t| five-fold mean | five-fold std\")\n",
    "for md, (mean, std) in cv_acc_stats.items():\n",
    "    print(f\"d={md}\\t\\t| {np.round(mean, 3)}\\t\\t | {np.round(std, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:  My classifier's maximum depth was d=12; thus, depth=15 was not possible for my implementation; therefore, d=15 is actually d=12, thereby, implying that the depth that maximizes the five-fold cross-validation accuracy is d=12 given the improved statistics. The depth, d=10, would also work reasonably well given how closely it matches the statistics in d=12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.b** - Explain briefly how you implemented the depth limit functionality in\n",
    "your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: Children at the maximum depth were set to the common label if the leafs were nodes that represented attributes; otherwise, the tree was created per-usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.c** - Using the depth with the greatest cross-validation accuracy from your\n",
    "experiments: train your decision tree on the data/train.csv file. Report the\n",
    "accuracy of your decision tree on the data/test.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: The ID3 algorithm implementation I wrote produced a tree of depth d=12. I had already experimented w/ the train and test datasets (Part 2 - Q2). The accuracy of my implementation at the optimal depth is 100.0% for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.e - 3.f**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: My depth limited tree was the full decision tree. However, it is clear that trees at depth d=10 could perform reasonably well. Clearly, a smaller tree could substantially reduce the computational demands during training and during prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cade_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
